{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning and Decision Making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laboratory 2: Markov decision problems\n",
    "\n",
    "In the end of the lab, you should export the notebook to a Python script (File >> Download as >> Python (.py)). Your file should be named `padi-lab2-groupXX.py`, where the `XX` corresponds to your group number and should be submitted to the e-mail <adi.tecnico@gmail.com>. \n",
    "\n",
    "Make sure...\n",
    "\n",
    "* **... that the subject is of the form `[<group n.>] LAB <lab n.>`.** \n",
    "\n",
    "* **... to strictly respect the specifications in each activity, in terms of the intended inputs, outputs and naming conventions.** \n",
    "\n",
    "In particular, after completing the activities you should be able to replicate the examples provided (although this, in itself, is no guarantee that the activities are correctly completed).\n",
    "\n",
    "### 1. The MDP Model\n",
    "\n",
    "Consider once again the Pacman modeling problem described in the Homework and for which you wrote a Markov decision problem model. In this lab, you will consider a larger version of the Pacman problem, described by the diagram:\n",
    "\n",
    "<img src=\"pacman-big.png\">\n",
    "\n",
    "Recall that the MDP should describe the decision-making of a player. In the above domain,\n",
    "\n",
    "* The ghost **moves randomly between cells 1-3**.\n",
    "* The player controls the movement of Pacman through four actions: `Up`, `Down`, `Left`, and `Right`. \n",
    "* Each action moves the Pacman character one step in the corresponding direction, if an adjacent cell exists in that direction. Otherwise, Pacman remains in the same place.\n",
    "* The cell in the bottom left corner (cell `29`) is adjacent, to the left, to the cell in the bottom right corner (cell `35`). In other words, if Pacman \"moves left\" in cell `29` it will end up in cell `35` and vice-versa.\n",
    "* If Pacman lies in the same cell as the ghost (in either cell `1`, `2`, or `3`), the player loses the game. However, if Pacman \"eats\" the blue pellet (in cell `24`), it gains the ability to \"eat\" the ghost. In this case, if Pacman lies in the same cell as the ghost, it \"eats\" the ghost and wins the game. Assume that Pacman can never be in cell `24` without \"eating\" the pellet.\n",
    "\n",
    "In this lab you will use an MDP based on the aforementioned domain and investigate how to evaluate, solve and simulate a Markov decision problem.\n",
    "\n",
    "**Throughout the lab, unless if stated otherwise, use $\\gamma=0.9$.**\n",
    "\n",
    "$$\\diamond$$\n",
    "\n",
    "In this first activity, you will implement an MDP model in Python. You will start by loading the MDP information from a `numpy` binary file, using the `numpy` function `load`. The file contains the list of states, actions, the transition probability matrices and cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 1.        \n",
    "\n",
    "Write a function named `load_mdp` that receives, as input, a string corresponding to the name of the file with the MDP information, and a real number $\\gamma$ between $0$ and $1$. The loaded file contains 4 arrays:\n",
    "\n",
    "* An array `X` that contains all the states in the MDP represented as strings. In the Pacman environment above, for example, there is a total of 209 states, each describing the position of Pacman in the environment, whether it has eaten the blue pellet, and the position of the ghost. Those states are either one of the strings `\"V\"` or `\"D\"`, corresponding to the absorbing \"victory\" and \"defeat\" states, or a string of the form `\"(p, s, g)\"`, where:\n",
    "    * `p` is a number between 1 and 35 indicating the position of Pacman;\n",
    "    * `s` is either `0` or `S`, where `0` indicates that Pacman has not yet eaten the pellet; `S` indicates that Pacman has eaten the pellet (and now has \"superpowers\");\n",
    "    * `g` is a number between 1 and 3, indicating the position of the ghost.\n",
    "* An array `A` that contains all the actions in the MDP represented as strings. In the Pacman environment above, for example, each action is represented as a string `\"Up\"`, `\"Down\"`, `\"Left\"` or `\"Right\"`.\n",
    "* An array `P` containing `len(A)` subarrays, each with dimension `len(X)` &times; `len(X)` and  corresponding to the transition probability matrix for one action.$^1$\n",
    "* An array `c` containing the cost function for the MDP.\n",
    "\n",
    "Your function should create the MDP as a tuple `(X, A, (Pa, a = 0, ..., len(A)), c, g)`, where `X` is a tuple containing the states in the MDP represented as strings (see above), `A` is a tuple containing the actions in the MDP represented as strings (see above), `P` is a tuple with `len(A)` elements, where `P[a]` is an `np.array` corresponding to the transition probability matrix for action `a`, `c` is an np.array corresponding to the cost function for the MDP, and `g` is a float, corresponding to the discount and provided as the argument $\\gamma$ of your function. Your function should return the MDP tuple.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-26T15:00:38.063248Z",
     "start_time": "2022-03-26T15:00:37.798375Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Insert your code here.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def load_mdp(MDP_file, gamma):\n",
    "    \n",
    "    mdp_info = np.load(MDP_file)\n",
    "    \n",
    "    P = mdp_info['P']\n",
    "    \n",
    "    Pa = () \n",
    "    for a in range(len(mdp_info['A'])):\n",
    "        Pa += (P[a], ) # append transition probability matrix associated with action a \n",
    "    \n",
    "    mdp = (tuple(mdp_info['X']), tuple(mdp_info['A']), Pa, mdp_info['c'], gamma)\n",
    "    \n",
    "    return mdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "= State space (209 states) =\n",
      "\n",
      "States:\n",
      "(1, S, 1)\n",
      "(1, S, 2)\n",
      "(1, S, 3)\n",
      "(1, 0, 1)\n",
      "(1, 0, 2)\n",
      "(1, 0, 3)\n",
      "(2, S, 1)\n",
      "(2, S, 2)\n",
      "(2, S, 3)\n",
      "(2, 0, 1)\n",
      "...\n",
      "\n",
      "Random state: s = (18, S, 1)\n",
      "\n",
      "Last state: D\n",
      "= Action space (4 actions) =\n",
      "Up\n",
      "Down\n",
      "Left\n",
      "Right\n",
      "\n",
      "Random action: a = Right\n",
      "\n",
      "= Transition probabilities =\n",
      "\n",
      "Transition probability matrix dimensions (action Up): (209, 209)\n",
      "Dimensions add up for action \"Up\"? True\n",
      "\n",
      "Transition probability matrix dimensions (action Down): (209, 209)\n",
      "Dimensions add up for action \"Down\"? True\n",
      "\n",
      "Transition probability matrix dimensions (action Left): (209, 209)\n",
      "Dimensions add up for action \"Left\"? True\n",
      "\n",
      "Transition probability matrix dimensions (action Right): (209, 209)\n",
      "Dimensions add up for action \"Right\"? True\n",
      "\n",
      "State-action pair ((18, S, 1), Right) transitions to state(s)\n",
      "s' in ['(19, S, 2)']\n",
      "\n",
      "= Costs =\n",
      "\n",
      "Special states with cost different from 0.1:\n",
      "['(1, S, 1)' '(1, 0, 1)' '(2, S, 2)' '(2, 0, 2)' '(3, S, 3)' '(3, 0, 3)'\n",
      " 'V' 'D']\n",
      "Associated costs:\n",
      "[[0. 1. 0. 1. 0. 1. 0. 0.]]\n",
      "\n",
      "Cost for the state-action pair ((18, S, 1), Right):\n",
      "c(s, a) = 0.1\n",
      "\n",
      "= Discount =\n",
      "\n",
      "gamma = 0.9\n"
     ]
    }
   ],
   "source": [
    "import numpy.random as rand\n",
    "\n",
    "M = load_mdp('pacman.npz', 0.9)\n",
    "\n",
    "rand.seed(42)\n",
    "\n",
    "# States\n",
    "print('= State space (%i states) =' % len(M[0]))\n",
    "print('\\nStates:')\n",
    "for i in range(min(10, len(M[0]))):\n",
    "    print(M[0][i]) \n",
    "\n",
    "print('...')\n",
    "\n",
    "# Random state\n",
    "s = rand.randint(len(M[0]))\n",
    "print('\\nRandom state: s =', M[0][s])\n",
    "\n",
    "# Last state\n",
    "print('\\nLast state:', M[0][-1])\n",
    "\n",
    "# Actions\n",
    "print('= Action space (%i actions) =' % len(M[1]))\n",
    "for i in range(len(M[1])):\n",
    "    print(M[1][i]) \n",
    "\n",
    "# Random action\n",
    "a = rand.randint(len(M[1]))\n",
    "print('\\nRandom action: a =', M[1][a])\n",
    "\n",
    "# Transition probabilities\n",
    "print('\\n= Transition probabilities =')\n",
    "\n",
    "for i in range(len(M[1])):\n",
    "    print('\\nTransition probability matrix dimensions (action %s):' % M[1][i], M[2][i].shape)\n",
    "    print('Dimensions add up for action \"%s\"?' % M[1][i], np.isclose(np.sum(M[2][i]), len(M[0])))\n",
    "\n",
    "print('\\nState-action pair (%s, %s) transitions to state(s)' % (M[0][s], M[1][a]))\n",
    "print(\"s' in\", np.array(M[0])[np.where(M[2][a][s, :] > 0)])\n",
    "\n",
    "# Cost\n",
    "print('\\n= Costs =')\n",
    "\n",
    "print('\\nSpecial states with cost different from 0.1:')\n",
    "print(np.array(M[0])[np.where(M[3][:, 0] != 0.1)])\n",
    "print('Associated costs:')\n",
    "print(M[3][np.where(M[3][:, 0] != 0.1), 0])\n",
    "\n",
    "print('\\nCost for the state-action pair (%s, %s):' % (M[0][s], M[1][a]))\n",
    "print('c(s, a) =', M[3][s, a])\n",
    "\n",
    "\n",
    "# Discount\n",
    "print('\\n= Discount =')\n",
    "print('\\ngamma =', M[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide below an example of application of the function with the file `pacman.npz` that you can use as a first \"sanity check\" for your code. Note that, even fixing the seed, the results you obtain may slightly differ.\n",
    "\n",
    "```python\n",
    "import numpy.random as rand\n",
    "\n",
    "M = load_mdp('pacman.npz', 0.9)\n",
    "\n",
    "rand.seed(42)\n",
    "\n",
    "# States\n",
    "print('= State space (%i states) =' % len(M[0]))\n",
    "print('\\nStates:')\n",
    "for i in range(min(10, len(M[0]))):\n",
    "    print(M[0][i]) \n",
    "\n",
    "print('...')\n",
    "\n",
    "# Random state\n",
    "s = rand.randint(len(M[0]))\n",
    "print('\\nRandom state: s =', M[0][s])\n",
    "\n",
    "# Last state\n",
    "print('\\nLast state:', M[0][-1])\n",
    "\n",
    "# Actions\n",
    "print('= Action space (%i actions) =' % len(M[1]))\n",
    "for i in range(len(M[1])):\n",
    "    print(M[1][i]) \n",
    "\n",
    "# Random action\n",
    "a = rand.randint(len(M[1]))\n",
    "print('\\nRandom action: a =', M[1][a])\n",
    "\n",
    "# Transition probabilities\n",
    "print('\\n= Transition probabilities =')\n",
    "\n",
    "for i in range(len(M[1])):\n",
    "    print('\\nTransition probability matrix dimensions (action %s):' % M[1][i], M[2][i].shape)\n",
    "    print('Dimensions add up for action \"%s\"?' % M[1][i], np.isclose(np.sum(M[2][i]), len(M[0])))\n",
    "    \n",
    "print('\\nState-action pair (%s, %s) transitions to state(s)' % (M[0][s], M[1][a]))\n",
    "print(\"s' in\", np.array(M[0])[np.where(M[2][a][s, :] > 0)])\n",
    "\n",
    "# Cost\n",
    "print('\\n= Costs =')\n",
    "\n",
    "print('\\nSpecial states with cost different from 0.1:')\n",
    "print(np.array(M[0])[np.where(M[3][:, 0] != 0.1)])\n",
    "print('Associated costs:')\n",
    "print(M[3][np.where(M[3][:, 0] != 0.1), 0])\n",
    "\n",
    "print('\\nCost for the state-action pair (%s, %s):' % (M[0][s], M[1][a]))\n",
    "print('c(s, a) =', M[3][s, a])\n",
    "\n",
    "\n",
    "# Discount\n",
    "print('\\n= Discount =')\n",
    "print('\\ngamma =', M[4])\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "= State space (209 states) =\n",
    "\n",
    "States:\n",
    "(1, S, 1)\n",
    "(1, S, 2)\n",
    "(1, S, 3)\n",
    "(1, 0, 1)\n",
    "(1, 0, 2)\n",
    "(1, 0, 3)\n",
    "(2, S, 1)\n",
    "(2, S, 2)\n",
    "(2, S, 3)\n",
    "(2, 0, 1)\n",
    "...\n",
    "\n",
    "Random state: s = (18, S, 1)\n",
    "\n",
    "Last state: D\n",
    "= Action space (4 actions) =\n",
    "Up\n",
    "Down\n",
    "Left\n",
    "Right\n",
    "\n",
    "Random action: a = Right\n",
    "\n",
    "= Transition probabilities =\n",
    "\n",
    "Transition probability matrix dimensions (action Up): (209, 209)\n",
    "Dimensions add up for action \"Up\"? True\n",
    "\n",
    "Transition probability matrix dimensions (action Down): (209, 209)\n",
    "Dimensions add up for action \"Down\"? True\n",
    "\n",
    "Transition probability matrix dimensions (action Left): (209, 209)\n",
    "Dimensions add up for action \"Left\"? True\n",
    "\n",
    "Transition probability matrix dimensions (action Right): (209, 209)\n",
    "Dimensions add up for action \"Right\"? True\n",
    "\n",
    "State-action pair ((18, S, 1), Right) transitions to state(s)\n",
    "s' in ['(19, S, 2)']\n",
    "\n",
    "= Costs =\n",
    "\n",
    "Special states with cost different from 0.1:\n",
    "['(1, S, 1)' '(1, 0, 1)' '(2, S, 2)' '(2, 0, 2)' '(3, S, 3)' '(3, 0, 3)'\n",
    " 'V' 'D']\n",
    "Associated costs:\n",
    "[[0. 1. 0. 1. 0. 1. 0. 0.]]\n",
    "\n",
    "Cost for the state-action pair ((18, S, 1), Right):\n",
    "c(s, a) = 0.1\n",
    "\n",
    "= Discount =\n",
    "\n",
    "gamma = 0.9\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prediction\n",
    "\n",
    "You are now going to evaluate a given policy, computing the corresponding cost-to-go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/latex"
   },
   "source": [
    "---\n",
    "\n",
    "#### Activity 2.\n",
    "\n",
    "Write a function `noisy_policy` that builds a noisy policy \"around\" a provided action. Your function should receive, as input, an MDP described as a tuple like that of **Activity 1**, an integer `a`, corresponding to the index of an action in the MDP, and a real number `eps`. The function should return, as output, a policy for the provided MDP that selects action with index `a` with a probability `1-eps` and, with probability `eps`, selects another action uniformly at random. The policy should be a `numpy` array with as many rows as states and as many columns as actions, where the element in position `[s,a]` should contain the probability of action `a` in state `s` according to the desired policy.\n",
    "\n",
    "**Note:** The examples provided correspond for the MDP in the previous Pacman environment. However, your code should be tested with MDPs of different sizes, so **make sure not to hard-code any of the MDP elements into your code**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-26T15:00:38.073641Z",
     "start_time": "2022-03-26T15:00:38.065456Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Insert your code here.\n",
    "def noisy_policy(mdp, a, eps):\n",
    "    \n",
    "    # probability distribution that selects action a with probability \n",
    "    # 1-eps, with the remaining actions being selected uniformaly at \n",
    "    # random (thus the factor eps/(len(mdp[1])-1), dividing by the \n",
    "    # remaining actions)\n",
    "    dist_prob = np.ones((1,len(mdp[1])))*eps/(len(mdp[1])-1)\n",
    "    dist_prob[0,a] = 1-eps\n",
    "    \n",
    "    # described policy, having the same probability distribution over the actions for every state \n",
    "    policy = np.repeat(dist_prob, len(mdp[0]), axis = 0)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random state: (18, 0, 2)\n",
      "Noiseless policy at selected state: [0. 0. 1. 0.]\n",
      "Noisy policy at selected state: [0.03 0.03 0.9  0.03]\n",
      "Random policy at selected state: [0.25 0.25 0.25 0.25]\n"
     ]
    }
   ],
   "source": [
    "rand.seed(42)\n",
    "\n",
    "# Noiseless policy for action \"Left\" (action index: 2)\n",
    "pol_noiseless = noisy_policy(M, 2, 0.)\n",
    "\n",
    "# Random state\n",
    "s = 106 # State (18, 0, 2)\n",
    "\n",
    "# Policy at selected state\n",
    "print('Random state:', M[0][s])\n",
    "print('Noiseless policy at selected state:', pol_noiseless[s, :])\n",
    "\n",
    "# Noisy policy for action \"Left\" (action index: 2)\n",
    "pol_noisy = noisy_policy(M, 2, 0.1)\n",
    "\n",
    "# Policy at selected state\n",
    "print('Noisy policy at selected state:', np.round(pol_noisy[s, :], 2))\n",
    "\n",
    "# Random policy for action \"Left\" (action index: 2)\n",
    "pol_random = noisy_policy(M, 2, 0.75)\n",
    "\n",
    "# Policy at selected state\n",
    "print('Random policy at selected state:', np.round(pol_random[s, :], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide below an example of application of the function with MDP from the example in **Activity 1**, that you can use as a first \"sanity check\" for your code. Note that, even fixing the seed, the results you obtain may slightly differ. Note also that, as emphasized above, your function should work with **any** MDP that is specified as a tuple with the structure of the one from **Activity 1**.\n",
    "\n",
    "```python\n",
    "rand.seed(42)\n",
    "\n",
    "# Noiseless policy for action \"Left\" (action index: 2)\n",
    "pol_noiseless = noisy_policy(M, 2, 0.)\n",
    "\n",
    "# Random state\n",
    "s = 106 # State (18, 0, 2)\n",
    "\n",
    "# Policy at selected state\n",
    "print('Random state:', M[0][s])\n",
    "print('Noiseless policy at selected state:', pol_noiseless[s, :])\n",
    "\n",
    "# Noisy policy for action \"Left\" (action index: 2)\n",
    "pol_noisy = noisy_policy(M, 2, 0.1)\n",
    "\n",
    "# Policy at selected state\n",
    "print('Noisy policy at selected state:', np.round(pol_noisy[s, :], 2))\n",
    "\n",
    "# Random policy for action \"Left\" (action index: 2)\n",
    "pol_random = noisy_policy(M, 2, 0.75)\n",
    "\n",
    "# Policy at selected state\n",
    "print('Random policy at selected state:', np.round(pol_random[s, :], 2))\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "Random state: (18, 0, 2)\n",
    "Noiseless policy at selected state: [0. 0. 1. 0.]\n",
    "Noisy policy at selected state: [0.03 0.03 0.9  0.03]\n",
    "Random policy at selected state: [0.25 0.25 0.25 0.25]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 3.\n",
    "\n",
    "You will now write a function called `evaluate_pol` that evaluates a given policy. Your function should receive, as an input, an MDP described as a tuple like that of **Activity 1** and a policy described as an array like that of **Activity 2** and return a `numpy` array corresponding to the cost-to-go function associated with the given policy. \n",
    "\n",
    "**Note:** The array returned by your function should have as many rows as the number of states in the received MDP, and exactly one column. Note also that, as before, your function should work with **any** MDP that is specified as a tuple with the same structure as the one from **Activity 1**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-26T15:00:38.110500Z",
     "start_time": "2022-03-26T15:00:38.078721Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Insert your code here.\n",
    "def evaluate_pol(mdp, policy):\n",
    "    \n",
    "    # initialization of the averaged policy transition probability\n",
    "    Ppi = np.zeros((len(mdp[0]), (len(mdp[0]))))\n",
    "    \n",
    "    # immediate costs\n",
    "    c = mdp[-2]\n",
    "    \n",
    "    # policy average cost, computed by multiplying element-wise the immediate costs by the policy, \n",
    "    # summing over the columns\n",
    "    cpi = (c * policy).sum(axis=1)\n",
    "    \n",
    "    # discounted factor \n",
    "    gamma = mdp[-1]\n",
    "    \n",
    "    for a in range(len(mdp[1])):\n",
    "        \n",
    "        # trasition probability matrix associated with action a \n",
    "        Pa = mdp[2][a]\n",
    "        \n",
    "        Ppi += np.diag(policy[:,a]).dot(Pa)\n",
    "            \n",
    "    Jpi = np.linalg.inv(np.eye(len(mdp[0])) - gamma * Ppi).dot(cpi)\n",
    "\n",
    "    return Jpi[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of cost-to-go: (209, 1)\n",
      "\n",
      "Example values of the computed cost-to-go:\n",
      "\n",
      "Cost-to-go at state (18, 0, 2): [1.]\n",
      "Cost-to-go at state (3, S, 1): [0.144]\n",
      "Cost-to-go at state (28, 0, 3): [1.]\n",
      "\n",
      "Example values of the computed cost-to-go:\n",
      "\n",
      "Cost-to-go at state (18, 0, 2): [1.]\n",
      "Cost-to-go at state (3, S, 1): [0.905]\n",
      "Cost-to-go at state (28, 0, 3): [1.]\n"
     ]
    }
   ],
   "source": [
    "Jact2 = evaluate_pol(M, pol_noisy)\n",
    "\n",
    "rand.seed(42)\n",
    "\n",
    "print('Dimensions of cost-to-go:', Jact2.shape)\n",
    "\n",
    "print('\\nExample values of the computed cost-to-go:')\n",
    "\n",
    "s = 106 # State (18, 0, 2)\n",
    "print('\\nCost-to-go at state %s:' % M[0][s], np.round(Jact2[s], 3))\n",
    "\n",
    "s = 12 # State (3, S, 1)\n",
    "print('Cost-to-go at state %s:' % M[0][s], np.round(Jact2[s], 3))\n",
    "\n",
    "s = 164 # State (28, 0, 3)\n",
    "print('Cost-to-go at state %s:' % M[0][s], np.round(Jact2[s], 3))\n",
    "\n",
    "# Example with random policy\n",
    "\n",
    "rand_pol = rand.randint(2, size=(len(M[0]), len(M[1]))) + 0.01 # We add 0.01 to avoid all-zero rows\n",
    "rand_pol = rand_pol / rand_pol.sum(axis = 1, keepdims = True)\n",
    "\n",
    "Jrand = evaluate_pol(M, rand_pol)\n",
    "\n",
    "print('\\nExample values of the computed cost-to-go:')\n",
    "\n",
    "s = 106 # State (18, 0, 2)\n",
    "print('\\nCost-to-go at state %s:' % M[0][s], np.round(Jrand[s], 3))\n",
    "\n",
    "s = 12 # State (3, S, 1)\n",
    "print('Cost-to-go at state %s:' % M[0][s], np.round(Jrand[s], 3))\n",
    "\n",
    "s = 164 # State (28, 0, 3)\n",
    "print('Cost-to-go at state %s:' % M[0][s], np.round(Jrand[s], 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, you can evaluate the random policy from **Activity 2** in the MDP from **Activity 1**.\n",
    "\n",
    "```python\n",
    "Jact2 = evaluate_pol(M, pol_noisy)\n",
    "\n",
    "rand.seed(42)\n",
    "\n",
    "print('Dimensions of cost-to-go:', Jact2.shape)\n",
    "\n",
    "print('\\nExample values of the computed cost-to-go:')\n",
    "\n",
    "s = 106 # State (18, 0, 2)\n",
    "print('\\nCost-to-go at state %s:' % M[0][s], np.round(Jact2[s], 3))\n",
    "\n",
    "s = 12 # State (3, S, 1)\n",
    "print('Cost-to-go at state %s:' % M[0][s], np.round(Jact2[s], 3))\n",
    "\n",
    "s = 164 # State (28, 0, 3)\n",
    "print('Cost-to-go at state %s:' % M[0][s], np.round(Jact2[s], 3))\n",
    "\n",
    "# Example with random policy\n",
    "\n",
    "rand_pol = rand.randint(2, size=(len(M[0]), len(M[1]))) + 0.01 # We add 0.01 to avoid all-zero rows\n",
    "rand_pol = rand_pol / rand_pol.sum(axis = 1, keepdims = True)\n",
    "\n",
    "Jrand = evaluate_pol(M, rand_pol)\n",
    "\n",
    "print('\\nExample values of the computed cost-to-go:')\n",
    "\n",
    "s = 106 # State (18, 0, 2)\n",
    "print('\\nCost-to-go at state %s:' % M[0][s], np.round(Jrand[s], 3))\n",
    "\n",
    "s = 12 # State (3, S, 1)\n",
    "print('Cost-to-go at state %s:' % M[0][s], np.round(Jrand[s], 3))\n",
    "\n",
    "s = 164 # State (28, 0, 3)\n",
    "print('Cost-to-go at state %s:' % M[0][s], np.round(Jrand[s], 3))\n",
    "```\n",
    "\n",
    "Output: \n",
    "```\n",
    "Dimensions of cost-to-go: (209, 1)\n",
    "\n",
    "Example values of the computed cost-to-go:\n",
    "\n",
    "Cost-to-go at state (18, 0, 2): [1.]\n",
    "Cost-to-go at state (3, S, 1): [0.144]\n",
    "Cost-to-go at state (28, 0, 3): [1.]\n",
    "\n",
    "Example values of the computed cost-to-go:\n",
    "\n",
    "Cost-to-go at state (18, 0, 2): [1.]\n",
    "Cost-to-go at state (3, S, 1): [0.905]\n",
    "Cost-to-go at state (28, 0, 3): [1.]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Control\n",
    "\n",
    "In this section you are going to compare value and policy iteration, both in terms of time and number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 4\n",
    "\n",
    "In this activity you will show that the policy in Activity 3 is _not_ optimal. For that purpose, you will use value iteration to compute the optimal cost-to-go, $J^*$, and show that $J^*\\neq J^\\pi$. \n",
    "\n",
    "Write a function called `value_iteration` that receives as input an MDP represented as a tuple like that of **Activity 1** and returns an `numpy` array corresponding to the optimal cost-to-go function associated with that MDP. Before returning, your function should print:\n",
    "\n",
    "* The time it took to run, in the format `Execution time: xxx seconds`, where `xxx` represents the number of seconds rounded up to $3$ decimal places.\n",
    "* The number of iterations, in the format `N. iterations: xxx`, where `xxx` represents the number of iterations.\n",
    "\n",
    "**Note 1:** Stop the algorithm when the error between iterations is smaller than $10^{-8}$.\n",
    "\n",
    "**Note 2:** You may find useful the function ``time()`` from the module ``time``.\n",
    "\n",
    "**Note 3:** The array returned by your function should have as many rows as the number of states in the received MDP, and exactly one column. As before, your function should work with **any** MDP that is specified as a tuple with the same structure as the one from **Activity 1**.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-26T15:00:38.138810Z",
     "start_time": "2022-03-26T15:00:38.113894Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Insert your code here.\n",
    "import time\n",
    "\n",
    "def value_iteration(mdp):\n",
    "    \n",
    "    num_states = len(mdp[0])\n",
    "    num_actions = len(mdp[1])\n",
    "    \n",
    "    # initialization of the discounted cost-to-go function associated \n",
    "    # with the optimal policy\n",
    "    Jpi_star = np.zeros((num_states, 1))\n",
    "    \n",
    "    # initialize norm between successive discounted cost-to-go functions,\n",
    "    # in such a way that a first iteration is performed \n",
    "    err = 1 \n",
    "    \n",
    "    # initialize number of iterations \n",
    "    i = 0 \n",
    "    \n",
    "    # immediate costs \n",
    "    c = mdp[-2]\n",
    "    \n",
    "    # discounted factor \n",
    "    gamma = mdp[-1]\n",
    "    \n",
    "    # start time counter \n",
    "    t = time.time() \n",
    "    \n",
    "    # initialization of the Q function associated with the optimal policy\n",
    "    Qpi = np.zeros((num_states, num_actions))\n",
    "        \n",
    "    # while the euclidean distance between successive discounter cost-to-go \n",
    "    # is larger than the error tolerance (has not converge to the solution), \n",
    "    # perform the value iteration algorithm\n",
    "    while err > 1e-8:\n",
    "        \n",
    "        for a in range(num_actions):\n",
    "            \n",
    "            # trasition probability matrix associated with action a \n",
    "            Pa = mdp[2][a]\n",
    "            \n",
    "            Qpi[:,a, None] = c[:,a, None] + gamma*Pa.dot(Jpi_star)\n",
    "        \n",
    "        # take the minimum element-wise \n",
    "        Jpi_new = np.min(Qpi, axis = 1)[:, None]\n",
    "        \n",
    "        # euclidean distance between the updated optimal discounted cost-to-go function and \n",
    "        # the previous estimate\n",
    "        err = np.linalg.norm(Jpi_new-Jpi_star)\n",
    "        \n",
    "        # incrementing the number of iterations \n",
    "        i += 1 \n",
    "        \n",
    "        # updating the discounted cost-to-go function    \n",
    "        Jpi_star = Jpi_new \n",
    "    \n",
    "    # execution time of the value iteration algorithm \n",
    "    t1 = time.time() - t\n",
    "    \n",
    "    print(f\"Execution time: {round(t1, 3)} seconds\")\n",
    "\n",
    "    print(f\"N. iterations: {i}\")\n",
    "    \n",
    "    return Jpi_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.005 seconds\n",
      "N. iterations: 23\n",
      "\n",
      "Dimensions of cost-to-go: (209, 1)\n",
      "\n",
      "Example values of the optimal cost-to-go:\n",
      "\n",
      "Cost to go at state (18, 0, 2): [0.75852275]\n",
      "Cost to go at state (3, S, 1): [0.1]\n",
      "Cost to go at state (28, 0, 3): [0.66875548]\n",
      "\n",
      "Is the policy from Activity 2 optimal? False\n"
     ]
    }
   ],
   "source": [
    "Jopt = value_iteration(M)\n",
    "\n",
    "print('\\nDimensions of cost-to-go:', Jopt.shape)\n",
    "\n",
    "rand.seed(42)\n",
    "\n",
    "print('\\nExample values of the optimal cost-to-go:')\n",
    "\n",
    "s = 106 # State (18, 0, 2)\n",
    "print('\\nCost to go at state %s:' % M[0][s], Jopt[s])\n",
    "\n",
    "s = 12 # State (3, S, 1)\n",
    "print('Cost to go at state %s:' % M[0][s], Jopt[s])\n",
    "\n",
    "s = 164 # State (28, 0, 3)\n",
    "print('Cost to go at state %s:' % M[0][s], Jopt[s])\n",
    "\n",
    "print('\\nIs the policy from Activity 2 optimal?', np.all(np.isclose(Jopt, Jact2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, using the MDP from **Activity 1** you could obtain the following interaction.\n",
    "\n",
    "```python\n",
    "Jopt = value_iteration(M)\n",
    "\n",
    "print('\\nDimensions of cost-to-go:', Jopt.shape)\n",
    "\n",
    "rand.seed(42)\n",
    "\n",
    "print('\\nExample values of the optimal cost-to-go:')\n",
    "\n",
    "s = 106 # State (18, 0, 2)\n",
    "print('\\nCost to go at state %s:' % M[0][s], Jopt[s])\n",
    "\n",
    "s = 12 # State (3, S, 1)\n",
    "print('Cost to go at state %s:' % M[0][s], Jopt[s])\n",
    "\n",
    "s = 164 # State (28, 0, 3)\n",
    "print('Cost to go at state %s:' % M[0][s], Jopt[s])\n",
    "\n",
    "print('\\nIs the policy from Activity 2 optimal?', np.all(np.isclose(Jopt, Jact2)))\n",
    "```\n",
    "\n",
    "Output:\n",
    "```\n",
    "Execution time: 0.007 seconds\n",
    "N. iterations: 23\n",
    "\n",
    "Dimensions of cost-to-go: (209, 1)\n",
    "\n",
    "Example values of the optimal cost-to-go:\n",
    "\n",
    "Cost to go at state (18, 0, 2): [0.75852275]\n",
    "Cost to go at state (3, S, 1): [0.1]\n",
    "Cost to go at state (28, 0, 3): [0.66875548]\n",
    "\n",
    "Is the policy from Activity 2 optimal? False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 5\n",
    "\n",
    "You will now compute the optimal policy using policy iteration. Write a function called `policy_iteration` that receives as input an MDP represented as a tuple like that of **Activity 1** and returns an `numpy` array corresponding to the optimal policy associated with that MDP. Before returning, your function should print:\n",
    "* The time it took to run, in the format `Execution time: xxx seconds`, where `xxx` represents the number of seconds rounded up to $3$ decimal places.\n",
    "* The number of iterations, in the format `N. iterations: xxx`, where `xxx` represents the number of iterations.\n",
    "\n",
    "**Note:** If you find that numerical errors affect your computations (especially when comparing two values/arrays) you may use the `numpy` function `isclose` with adequately set absolute and relative tolerance parameters (e.g., $10^{-8}$).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-26T15:00:38.176379Z",
     "start_time": "2022-03-26T15:00:38.142084Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Insert your code here.\n",
    "def policy_iteration(mdp):\n",
    "    \n",
    "    num_states = len(mdp[0])\n",
    "    num_actions = len(mdp[1])\n",
    "    \n",
    "    # initialization of the optimal policy, giving in every state an equal \n",
    "    # probability of selecting action a\n",
    "    popt = np.ones((num_states, num_actions))*(1/num_actions)\n",
    "    \n",
    "    quit = False \n",
    "    \n",
    "    it = 0 # initialization of the number of iterations \n",
    "    \n",
    "    c = mdp[-2] # immediate costs \n",
    "    \n",
    "    gamma = mdp[-1]# discount factor\n",
    "    \n",
    "    # initizalization of the Q function \n",
    "    Q = np.zeros((num_states, num_actions))\n",
    "    \n",
    "    # start time counter\n",
    "    t = time.time()\n",
    "    \n",
    "    while not quit:\n",
    "        \n",
    "        # policy evaluation (solving the linear system)\n",
    "        J = evaluate_pol(mdp, popt)\n",
    "        \n",
    "        # policy improvement \n",
    "        for a in range(num_actions):\n",
    "\n",
    "            Pa = mdp[2][a]\n",
    "                        \n",
    "            Q[:,a, None] = c[:,a, None] + gamma*Pa.dot(J)\n",
    "        \n",
    "        \n",
    "        # compute minimizing policy:\n",
    "        \n",
    "        # np.min(Q, axis = 1) gives at each state the action that minimizes Q, \n",
    "        # that is, the greedy policy with respect to J (note that the optimal policy \n",
    "        # is associated with Q* and J*)\n",
    "        Qmin = np.min(Q, axis = 1, keepdims = True) \n",
    "        pinew = np.isclose(Q, Qmin, atol = 1e-10, rtol = 1e-10).astype(int) \n",
    "   \n",
    "        \n",
    "        # greedy policy with respect to the last one. at each state, \n",
    "        # we have a probability distribution over the actions, that is, \n",
    "        # the probabilities of selecting the available actions at each \n",
    "        # state have to sum 1\n",
    "        pinew = pinew / np.sum(pinew, axis = 1, keepdims = True)\n",
    "        \n",
    "        # if the updated policy and the previous estimate are the same, \n",
    "        # it must be the optimal policy (quit set to True, being False otherwise)\n",
    "        quit = (popt == pinew).all()\n",
    "        \n",
    "        # update solution of the optimal policy \n",
    "        popt = pinew \n",
    "        \n",
    "        # incrementation of the number of iterations \n",
    "        it += 1\n",
    "    \n",
    "    # execution time of the policy iteration algorithm \n",
    "    t1 = time.time() - t\n",
    "    \n",
    "    print(f\"Execution time: {round(t1, 3)} seconds\")\n",
    "    print(f\"N. iterations: {it}\")\n",
    "    \n",
    "    return popt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.013 seconds\n",
      "N. iterations: 3\n",
      "\n",
      "Dimension of the policy matrix: (209, 4)\n",
      "\n",
      "Examples of actions according to the optimal policy:\n",
      "Policy at state (18, 0, 2): Right\n",
      "Policy at state (3, S, 1): Left\n",
      "Policy at state (28, 0, 3): Up\n",
      "\n",
      "Optimality of the computed policy:\n",
      "- Is the new policy optimal? True\n"
     ]
    }
   ],
   "source": [
    "popt = policy_iteration(M)\n",
    "\n",
    "print('\\nDimension of the policy matrix:', popt.shape)\n",
    "\n",
    "rand.seed(42)\n",
    "\n",
    "print('\\nExamples of actions according to the optimal policy:')\n",
    "\n",
    "# Select random state, and action using the policy computed\n",
    "s = 106 # State (18, 0, 2)\n",
    "a = rand.choice(len(M[1]), p=popt[s, :])\n",
    "print('Policy at state %s: %s' % (M[0][s], M[1][a]))\n",
    "\n",
    "# Select random state, and action using the policy computed\n",
    "s = 12 # State (3, S, 1)\n",
    "a = rand.choice(len(M[1]), p=popt[s, :])\n",
    "print('Policy at state %s: %s' % (M[0][s], M[1][a]))\n",
    "\n",
    "# Select random state, and action using the policy computed\n",
    "s = 164 # State (28, 0, 3)\n",
    "a = rand.choice(len(M[1]), p=popt[s, :])\n",
    "print('Policy at state %s: %s' % (M[0][s], M[1][a]))\n",
    "\n",
    "# Verify optimality of the computed policy\n",
    "\n",
    "print('\\nOptimality of the computed policy:')\n",
    "\n",
    "Jpi = evaluate_pol(M, popt)\n",
    "print('- Is the new policy optimal?', np.all(np.isclose(Jopt, Jpi)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, using the MDP from **Activity 1** you could obtain the following interaction.\n",
    "\n",
    "```python\n",
    "popt = policy_iteration(M)\n",
    "\n",
    "print('\\nDimension of the policy matrix:', popt.shape)\n",
    "\n",
    "rand.seed(42)\n",
    "\n",
    "print('\\nExamples of actions according to the optimal policy:')\n",
    "\n",
    "# Select random state, and action using the policy computed\n",
    "s = 106 # State (18, 0, 2)\n",
    "a = rand.choice(len(M[1]), p=popt[s, :])\n",
    "print('Policy at state %s: %s' % (M[0][s], M[1][a]))\n",
    "\n",
    "# Select random state, and action using the policy computed\n",
    "s = 12 # State (3, S, 1)\n",
    "a = rand.choice(len(M[1]), p=popt[s, :])\n",
    "print('Policy at state %s: %s' % (M[0][s], M[1][a]))\n",
    "\n",
    "# Select random state, and action using the policy computed\n",
    "s = 164 # State (28, 0, 3)\n",
    "a = rand.choice(len(M[1]), p=popt[s, :])\n",
    "print('Policy at state %s: %s' % (M[0][s], M[1][a]))\n",
    "\n",
    "# Verify optimality of the computed policy\n",
    "\n",
    "print('\\nOptimality of the computed policy:')\n",
    "\n",
    "Jpi = evaluate_pol(M, popt)\n",
    "print('- Is the new policy optimal?', np.all(np.isclose(Jopt, Jpi)))\n",
    "```\n",
    "\n",
    "Output:\n",
    "```\n",
    "Execution time: 0.006 seconds\n",
    "N. iterations: 3\n",
    "\n",
    "Dimension of the policy matrix: (209, 4)\n",
    "\n",
    "Examples of actions according to the optimal policy:\n",
    "Policy at state (18, 0, 2): Right\n",
    "Policy at state (3, S, 1): Left\n",
    "Policy at state (28, 0, 3): Up\n",
    "\n",
    "Optimality of the computed policy:\n",
    "- Is the new policy optimal? True\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Simulation\n",
    "\n",
    "Finally, in this section you will check whether the theoretical computations of the cost-to-go actually correspond to the cost incurred by an agent following a policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 6\n",
    "\n",
    "Write a function `simulate` that receives, as inputs\n",
    "\n",
    "* An MDP represented as a tuple like that of **Activity 1**;\n",
    "* A policy, represented as an `numpy` array like that of **Activity 2**;\n",
    "* An integer, `x0`, corresponding to a state index\n",
    "* A second integer, `length`\n",
    "\n",
    "Your function should return, as an output, a float corresponding to the estimated cost-to-go associated with the provided policy at the provided state. To estimate such cost-to-go, your function should:\n",
    "\n",
    "* Generate **`NRUNS`** trajectories of `length` steps each, starting in the provided state and following the provided policy. \n",
    "* For each trajectory, compute the accumulated (discounted) cost. \n",
    "* Compute the average cost over the 100 trajectories.\n",
    "\n",
    "**Note 1:** You may find useful to import the numpy module `numpy.random`.\n",
    "\n",
    "**Note 2:** Each simulation may take a bit of time, don't despair ☺️.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-26T15:00:37.867Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "NRUNS = 100 # Do not delete this\n",
    "\n",
    "# Insert your code here.\n",
    "import math\n",
    "\n",
    "def simulate(mdp, policy, x0, length):\n",
    "    \n",
    "    num_states = len(mdp[0])\n",
    "    num_actions = len(mdp[1])\n",
    "    \n",
    "    # immediate costs \n",
    "    c = mdp[-2]\n",
    "    \n",
    "    # initialization of the accumulated discounted costs for each trajectory \n",
    "    accumulated_J = np.zeros((1,NRUNS))\n",
    "    \n",
    "    gamma = mdp[-1]\n",
    "    \n",
    "    # for each trajectory generated\n",
    "    for run in range(NRUNS):\n",
    "        \n",
    "        # present state, being initially defined by the provided x0\n",
    "        x = x0 \n",
    "         \n",
    "        for t in range(length):\n",
    "            \n",
    "            # Select random action, given the probability distribution at state x\n",
    "            a = rand.choice(num_actions, p=policy[x, :])\n",
    "            \n",
    "            # updating the accumulated discounted costs until ct for the corresponding \n",
    "            # trajectory\n",
    "            accumulated_J[0,run] +=(gamma**t)*c[x,a]\n",
    "            \n",
    "            # trasition probability matrix associated with action a \n",
    "            Pa = mdp[2][a]\n",
    "            \n",
    "            # Select random state, given the probability distribution at state x\n",
    "            x_next = rand.choice(num_states, p=Pa[x, :])\n",
    "            \n",
    "            # present state\n",
    "            x = x_next\n",
    "\n",
    "    # average cost over the NRUNS trajectories\n",
    "    average_J = np.sum(accumulated_J)/NRUNS\n",
    "\n",
    "    return average_J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost-to-go for state (18, 0, 2):\n",
      "\tTheoretical: [0.7585]\n",
      "\tEmpirical: 0.7588\n",
      "Cost-to-go for state (3, S, 1):\n",
      "\tTheoretical: [0.1]\n",
      "\tEmpirical: 0.1\n",
      "Cost-to-go for state (28, 0, 3):\n",
      "\tTheoretical: [0.6688]\n",
      "\tEmpirical: 0.6677\n"
     ]
    }
   ],
   "source": [
    "rand.seed(42)\n",
    "\n",
    "# Select random state, and evaluate for the optimal policy\n",
    "s = 106 # State (18, 0, 2)\n",
    "print('Cost-to-go for state %s:' % M[0][s])\n",
    "print('\\tTheoretical:', np.round(Jopt[s], 4))\n",
    "print('\\tEmpirical:', np.round(simulate(M, popt, s, 1000), 4))\n",
    "\n",
    "# Select random state, and evaluate for the optimal policy\n",
    "s = 12 # State (3, S, 1)\n",
    "print('Cost-to-go for state %s:' % M[0][s])\n",
    "print('\\tTheoretical:', np.round(Jopt[s], 4))\n",
    "print('\\tEmpirical:', np.round(simulate(M, popt, s, 1000), 4))\n",
    "\n",
    "# Select random state, and evaluate for the optimal policy\n",
    "s = 164 # State (28, 0, 3)\n",
    "print('Cost-to-go for state %s:' % M[0][s])\n",
    "print('\\tTheoretical:', np.round(Jopt[s], 4))\n",
    "print('\\tEmpirical:', np.round(simulate(M, popt, s, 1000), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we can use this function to estimate the values of some random states and compare them with those from **Activity 4**.\n",
    "\n",
    "```python\n",
    "rand.seed(42)\n",
    "\n",
    "# Select random state, and evaluate for the optimal policy\n",
    "s = 106 # State (18, 0, 2)\n",
    "print('Cost-to-go for state %s:' % M[0][s])\n",
    "print('\\tTheoretical:', np.round(Jopt[s], 4))\n",
    "print('\\tEmpirical:', np.round(simulate(M, popt, s, 1000), 4))\n",
    "\n",
    "# Select random state, and evaluate for the optimal policy\n",
    "s = 12 # State (3, S, 1)\n",
    "print('Cost-to-go for state %s:' % M[0][s])\n",
    "print('\\tTheoretical:', np.round(Jopt[s], 4))\n",
    "print('\\tEmpirical:', np.round(simulate(M, popt, s, 1000), 4))\n",
    "\n",
    "# Select random state, and evaluate for the optimal policy\n",
    "s = 164 # State (28, 0, 3)\n",
    "print('Cost-to-go for state %s:' % M[0][s])\n",
    "print('\\tTheoretical:', np.round(Jopt[s], 4))\n",
    "print('\\tEmpirical:', np.round(simulate(M, popt, s, 1000), 4))\n",
    "```\n",
    "\n",
    "Output:\n",
    "````\n",
    "Cost-to-go for state (18, 0, 2):\n",
    "\tTheoretical: [0.7585]\n",
    "\tEmpirical: 0.7588\n",
    "Cost-to-go for state (3, S, 1):\n",
    "\tTheoretical: [0.1]\n",
    "\tEmpirical: 0.1\n",
    "Cost-to-go for state (28, 0, 3):\n",
    "\tTheoretical: [0.6688]\n",
    "\tEmpirical: 0.6677\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
